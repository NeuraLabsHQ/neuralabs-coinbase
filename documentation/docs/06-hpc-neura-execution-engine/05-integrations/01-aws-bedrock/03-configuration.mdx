---
id: hpc-aws-bedrock-configuration
title: Configuration
sidebar_label: Configuration
sidebar_position: 3
---

import { Card, CardHeader, CardTitle, CardDescription } from '@site/src/components/Card';
import { Callout } from '@site/src/components/Callout';
import { Features, Feature } from '@site/src/components/Features';
import { CollapsibleCodeBlock, InlineCodeCard } from '@site/src/components/CodeBlock';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';

# AWS Bedrock Configuration

This guide covers advanced configuration options for AWS Bedrock integration in the HPC Neura Execution Engine, including model parameters, streaming settings, and optimization techniques.

## Configuration Levels

The HPC Execution Engine supports configuration at multiple levels:

<Card>
  <CardHeader>
    <CardTitle>Configuration Hierarchy</CardTitle>
  </CardHeader>
  <div style={{ padding: '1rem' }}>
    <ol>
      <li><strong>Environment Variables</strong> - Global defaults in `.env`</li>
      <li><strong>Flow Definition</strong> - Per-node configuration</li>
      <li><strong>Runtime Overrides</strong> - Dynamic configuration via API</li>
      <li><strong>Model-Specific Settings</strong> - Provider-specific parameters</li>
    </ol>
    
    <Callout type="info">
      Configuration follows a precedence order: Runtime Overrides > Flow Definition > Environment Variables > Defaults
    </Callout>
  </div>
</Card>

## Environment Configuration

### Basic Settings

<CollapsibleCodeBlock
  title="Essential Environment Variables"
  description="Core AWS Bedrock configuration in .env"
  language="bash"
  defaultCollapsed={false}
>
{`# AWS Credentials and Region
AWS_REGION=us-east-2
AWS_ACCESS_KEY_ID=your_access_key_id
AWS_SECRET_ACCESS_KEY=your_secret_access_key

# Default Model Settings
DEFAULT_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=1000

# Streaming Configuration
BEDROCK_STREAMING_ENABLED=true
BEDROCK_STREAM_CHUNK_SIZE=256

# Retry Configuration
BEDROCK_MAX_RETRIES=3
BEDROCK_RETRY_DELAY_MS=1000
BEDROCK_RETRY_MAX_DELAY_MS=10000

# Performance Settings
BEDROCK_CONNECTION_TIMEOUT=30
BEDROCK_READ_TIMEOUT=300
BEDROCK_KEEPALIVE_TIMEOUT=60`}
</CollapsibleCodeBlock>

### Advanced Settings

<CollapsibleCodeBlock
  title="Advanced Configuration Options"
  description="Performance and optimization settings"
  language="bash"
>
{`# Connection Pooling
BEDROCK_MAX_POOL_CONNECTIONS=50
BEDROCK_POOL_CONNECTION_TIMEOUT=10

# Request Configuration
BEDROCK_MAX_REQUEST_SIZE_MB=10
BEDROCK_RESPONSE_BUFFER_SIZE_KB=512

# Logging and Monitoring
BEDROCK_LOG_REQUESTS=true
BEDROCK_LOG_RESPONSES=false
BEDROCK_METRICS_ENABLED=true
BEDROCK_TRACE_ENABLED=false

# Cost Control
BEDROCK_DAILY_TOKEN_LIMIT=1000000
BEDROCK_ALERT_THRESHOLD_USD=100

# Model-Specific Overrides
BEDROCK_CLAUDE_DEFAULT_TEMPERATURE=0.5
BEDROCK_DEEPSEEK_DEFAULT_MAX_TOKENS=4000
BEDROCK_LLAMA_DEFAULT_TOP_P=0.9`}
</CollapsibleCodeBlock>

## Flow-Level Configuration

### Node Configuration

Configure individual nodes in your flow definition:

<CollapsibleCodeBlock
  title="Node-Level Model Configuration"
  description="Configure models within flow definitions"
  language="yaml"
>
{`nodes:
  # Basic configuration
  simple_ai:
    type: llm_text
    model: anthropic.claude-3-haiku-20240307-v1:0
    temperature: 0.7
    max_tokens: 1000
    
  # Advanced configuration
  advanced_ai:
    type: llm_structured
    model: anthropic.claude-3-opus-20240229-v1:0
    temperature: 0.3
    max_tokens: 4000
    top_p: 0.95
    top_k: 40
    stop_sequences: ["\\n\\n", "END"]
    
    # Provider-specific parameters
    anthropic_params:
      system: "You are a helpful assistant"
      metadata:
        user_id: "flow_user_123"
        
  # DeepSeek with special handling
  reasoning_ai:
    type: llm_text
    model: arn:aws:bedrock:us-east-2:account:inference-profile/us.deepseek.r1-v1:0
    temperature: 0.7
    max_tokens: 8000
    
    # DeepSeek-specific settings
    deepseek_params:
      reasoning_mode: true
      stream_reasoning: true`}
</CollapsibleCodeBlock>

### Streaming Configuration

<Tabs>
  <TabItem value="standard" label="Standard Streaming" default>
    <div style={{ marginTop: '1rem' }}>
      <CollapsibleCodeBlock
        title="Standard Streaming Setup"
        description="Configure streaming for real-time responses"
        language="yaml"
      >
{`nodes:
  streaming_ai:
    type: llm_text
    model: anthropic.claude-3-haiku-20240307-v1:0
    streaming: true  # Enable streaming
    
    # Streaming parameters
    stream_config:
      chunk_size: 256        # Bytes per chunk
      buffer_size: 1024      # Response buffer
      flush_interval: 100    # ms between flushes
      
    # Streaming optimizations
    stream_optimizations:
      early_termination: true
      compression: true
      keep_alive: true`}
      </CollapsibleCodeBlock>
    </div>
  </TabItem>
  
  <TabItem value="deepseek" label="DeepSeek Streaming">
    <div style={{ marginTop: '1rem' }}>
      <CollapsibleCodeBlock
        title="DeepSeek Reasoning Stream"
        description="Special handling for DeepSeek's reasoning traces"
        language="yaml"
      >
{`nodes:
  deepseek_reasoning:
    type: llm_text
    model: arn:aws:bedrock:us-east-2:account:inference-profile/us.deepseek.r1-v1:0
    streaming: true
    
    # DeepSeek streaming configuration
    deepseek_stream_config:
      include_reasoning: true
      reasoning_prefix: "<think>"
      reasoning_suffix: "</think>"
      answer_prefix: "<answer>"
      answer_suffix: "</answer>"
      
    # Process reasoning separately
    stream_handlers:
      on_reasoning_chunk: "process_reasoning"
      on_answer_chunk: "process_answer"`}
      </CollapsibleCodeBlock>
    </div>
  </TabItem>
</Tabs>

## Runtime Configuration

### Dynamic Model Selection

Override model selection at runtime:

<CollapsibleCodeBlock
  title="Runtime Configuration API"
  description="Override settings when executing flows"
  language="json"
>
{`{
  "flow_id": "flow_123",
  "flow_definition": { ... },
  "initial_inputs": { ... },
  
  // Runtime configuration
  "config": {
    // Global overrides
    "default_model": "anthropic.claude-3-sonnet-20240229-v1:0",
    "default_temperature": 0.5,
    
    // Per-node model overrides
    "model_overrides": {
      "analyzer_node": "anthropic.claude-3-opus-20240229-v1:0",
      "summarizer_node": "meta.llama3-70b-instruct-v1:0"
    },
    
    // Per-node parameter overrides
    "parameter_overrides": {
      "analyzer_node": {
        "temperature": 0.2,
        "max_tokens": 4000
      }
    },
    
    // Performance settings
    "performance": {
      "parallel_limit": 5,
      "timeout_seconds": 300,
      "retry_attempts": 3
    },
    
    // Cost controls
    "cost_limits": {
      "max_tokens_per_request": 2000,
      "max_total_cost_usd": 10.0
    }
  }
}`}
</CollapsibleCodeBlock>

### Session-Based Configuration

Maintain configuration across multiple flow executions:

<CollapsibleCodeBlock
  title="Session Configuration"
  description="Persistent settings for user sessions"
  language="python"
>
{`# Create session configuration
session_config = {
    "session_id": "user_session_123",
    "user_preferences": {
        "preferred_model": "anthropic.claude-3-haiku-20240307-v1:0",
        "language": "en",
        "response_style": "concise"
    },
    "model_config": {
        "temperature": 0.6,
        "max_tokens": 1500,
        "system_prompt": "Be helpful and concise"
    },
    "cost_tracking": {
        "budget_usd": 50.0,
        "alert_threshold": 0.8
    }
}

# Apply to flow execution
execution_request = {
    "flow_definition": flow_def,
    "session_config": session_config
}`}
</CollapsibleCodeBlock>

## Model-Specific Configuration

### Anthropic Claude Configuration

<Card>
  <CardHeader>
    <CardTitle>Claude-Specific Parameters</CardTitle>
  </CardHeader>
  <div style={{ padding: '1rem' }}>
    <CollapsibleCodeBlock
      title="Claude Advanced Configuration"
      description="Anthropic-specific model parameters"
      language="yaml"
    >
{`nodes:
  claude_advanced:
    type: llm_text
    model: anthropic.claude-3-opus-20240229-v1:0
    
    # Standard parameters
    temperature: 0.3
    max_tokens: 4000
    
    # Claude-specific parameters
    anthropic_params:
      # System prompt (Claude 3 only)
      system: |
        You are an expert analyst. Provide detailed, 
        well-structured responses with citations.
      
      # Response formatting
      stop_sequences: ["Human:", "\\n\\nHuman:"]
      
      # Metadata for tracking
      metadata:
        user_id: "user_123"
        session_id: "session_456"
        purpose: "analysis"
      
      # Claude 3 specific
      top_k: 5  # Only return top K most likely tokens
      
      # Safety settings
      anthropic_version: "2023-06-01"
      anthropic_beta: "messages-2023-12-15"`}
    </CollapsibleCodeBlock>
  </div>
</Card>

### DeepSeek Configuration

<Card>
  <CardHeader>
    <CardTitle>DeepSeek R1 Configuration</CardTitle>
  </CardHeader>
  <div style={{ padding: '1rem' }}>
    <CollapsibleCodeBlock
      title="DeepSeek Advanced Configuration"
      description="Configure DeepSeek's reasoning model"
      language="yaml"
    >
{`nodes:
  deepseek_reasoning:
    type: llm_text
    model: arn:aws:bedrock:us-east-2:account:inference-profile/us.deepseek.r1-v1:0
    
    # High token limit for reasoning
    max_tokens: 8000
    temperature: 0.7
    
    # DeepSeek-specific configuration
    deepseek_params:
      # Reasoning control
      enable_reasoning: true
      reasoning_depth: "deep"  # shallow, medium, deep
      
      # Output control
      include_confidence: true
      include_alternatives: false
      
      # Performance tuning
      early_stopping: false
      beam_width: 1
      
      # Special tokens
      reasoning_token: "<think>"
      answer_token: "<answer>"`}
    </CollapsibleCodeBlock>
  </div>
</Card>

## Performance Optimization

### Connection Pooling

<CollapsibleCodeBlock
  title="Connection Pool Configuration"
  description="Optimize connection reuse"
  language="python"
>
{`# In your initialization code
from botocore.config import Config

# Configure connection pooling
bedrock_config = Config(
    region_name='us-east-2',
    max_pool_connections=50,
    retries={
        'max_attempts': 3,
        'mode': 'adaptive'
    },
    connect_timeout=30,
    read_timeout=300,
    tcp_keepalive=True
)

# Apply to Bedrock client
bedrock_runtime = boto3.client(
    'bedrock-runtime',
    config=bedrock_config
)`}
</CollapsibleCodeBlock>

### Request Batching

<CollapsibleCodeBlock
  title="Batch Request Configuration"
  description="Process multiple requests efficiently"
  language="yaml"
>
{`# Flow configuration for batching
flow_config:
  execution:
    batch_enabled: true
    batch_size: 10
    batch_timeout_ms: 5000
    
  # Node-specific batching
  nodes:
    classifier:
      batch_config:
        enabled: true
        max_batch_size: 20
        wait_time_ms: 100
        
    processor:
      batch_config:
        enabled: false  # Disable for real-time needs`}
</CollapsibleCodeBlock>

## Monitoring and Logging

### Request Logging

<CollapsibleCodeBlock
  title="Logging Configuration"
  description="Configure detailed request/response logging"
  language="yaml"
>
{`# Logging configuration
logging:
  bedrock_requests:
    enabled: true
    level: INFO
    
    # What to log
    log_request_body: true
    log_response_body: false  # Can be large
    log_headers: true
    log_metrics: true
    
    # Sensitive data handling
    redact_patterns:
      - "api_key"
      - "password"
      - "ssn"
    
    # Log destinations
    destinations:
      - type: file
        path: /var/log/bedrock/requests.log
        rotation: daily
        
      - type: cloudwatch
        log_group: /aws/bedrock/requests
        stream_prefix: hpc-node`}
</CollapsibleCodeBlock>

### Metrics Collection

<Features>
  <Feature title="Token Usage" icon="/img/icons/ai-workflow.svg">
    Track input/output token consumption
  </Feature>
  <Feature title="Latency Metrics" icon="/img/icons/hpc.svg">
    Monitor response times per model
  </Feature>
  <Feature title="Cost Tracking" icon="/img/icons/database.svg">
    Real-time cost monitoring and alerts
  </Feature>
</Features>

## Security Configuration

<Card>
  <CardHeader>
    <CardTitle>Security Best Practices</CardTitle>
  </CardHeader>
  <div style={{ padding: '1rem' }}>
    <CollapsibleCodeBlock
      title="Security Configuration"
      description="Secure your Bedrock integration"
      language="yaml"
    >
{`security:
  # Credential management
  credentials:
    source: "secrets_manager"  # or "env", "iam_role"
    secret_name: "bedrock/credentials"
    rotation_enabled: true
    
  # Network security
  network:
    use_vpc_endpoint: true
    vpc_endpoint_id: "vpce-12345"
    private_dns_enabled: true
    
  # Data protection
  encryption:
    in_transit: true
    at_rest: true
    kms_key_id: "arn:aws:kms:region:account:key/..."
    
  # Access control
  access_control:
    ip_whitelist:
      - "10.0.0.0/8"
      - "192.168.0.0/16"
    require_mfa: true
    
  # Audit logging
  audit:
    cloudtrail_enabled: true
    log_all_requests: true
    retention_days: 90`}
    </CollapsibleCodeBlock>
  </div>
</Card>

## Troubleshooting Configuration Issues

<Callout type="tip" title="Configuration Debugging">
Enable debug logging to troubleshoot configuration issues:

```bash
export BEDROCK_DEBUG=true
export BEDROCK_LOG_LEVEL=DEBUG
export AWS_DEBUG=true
```
</Callout>

## Next Steps

<div style={{ display: 'grid', gridTemplateColumns: 'repeat(auto-fit, minmax(250px, 1fr))', gap: '1rem', marginTop: '1.5rem' }}>
  <Card>
    <CardHeader>
      <CardTitle>Troubleshooting</CardTitle>
      <CardDescription>
        Resolve common configuration issues
      </CardDescription>
    </CardHeader>
    <div style={{ padding: '1rem', paddingTop: 0 }}>
      <a href="./troubleshooting" style={{ textDecoration: 'none' }}>
        Fix issues →
      </a>
    </div>
  </Card>
  
  <Card>
    <CardHeader>
      <CardTitle>Model Selection</CardTitle>
      <CardDescription>
        Choose the right model
      </CardDescription>
    </CardHeader>
    <div style={{ padding: '1rem', paddingTop: 0 }}>
      <a href="./model-selection" style={{ textDecoration: 'none' }}>
        Select models →
      </a>
    </div>
  </Card>
</div>