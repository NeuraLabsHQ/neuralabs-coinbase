{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import yaml\n",
    "import json\n",
    "import asyncio\n",
    "import websockets\n",
    "import time\n",
    "\n",
    "\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows asyncio.run() inside Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flow from ./execution_flows/simple-ai-flow.yaml...\n",
      "Flow ID: simple_ai_flow\n"
     ]
    }
   ],
   "source": [
    "# Load the YAML flow file\n",
    "yaml_file_path = \"./execution_flows/simple-ai-flow.yaml\"  # Update this to your file location\n",
    "print(f\"Loading flow from {yaml_file_path}...\")\n",
    "\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "flow_id = yaml_data.get(\"flow_id\", \"flow-\" + yaml_file_path.split(\"/\")[-1].split(\".\")[0])\n",
    "print(f\"Flow ID: {flow_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple_ai_flow'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket URL: ws://localhost:8000/ws/flow/simple_ai_flow\n"
     ]
    }
   ],
   "source": [
    "websocket_url = \"ws://localhost:8000/ws/flow/\" + flow_id\n",
    "print(f\"WebSocket URL: {websocket_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flow_id': 'simple_ai_flow',\n",
       " 'flow_definition': {'flow_id': 'simple_ai_flow',\n",
       "  'elements': {'start_node': {'type': 'start',\n",
       "    'element_id': 'start_node',\n",
       "    'name': 'Start Block',\n",
       "    'description': 'Entry point of the flow',\n",
       "    'input_schema': {},\n",
       "    'output_schema': {}},\n",
       "   'chat_input': {'type': 'chat_input',\n",
       "    'element_id': 'chat_input',\n",
       "    'name': 'User Input',\n",
       "    'description': \"Captures the user's message\",\n",
       "    'input_schema': {'chat_input': {'type': 'string',\n",
       "      'description': 'The input provided by the user',\n",
       "      'required': True}},\n",
       "    'output_schema': {'chat_input': {'type': 'string',\n",
       "      'description': 'The input provided by the user',\n",
       "      'required': True}}},\n",
       "   'context_history': {'type': 'context_history',\n",
       "    'element_id': 'context_history',\n",
       "    'name': 'Conversation Context',\n",
       "    'description': 'Provides conversation history for context',\n",
       "    'input_schema': {'context_history': {'type': 'list',\n",
       "      'description': 'List of previous messages',\n",
       "      'required': False}},\n",
       "    'output_schema': {'context_history': {'type': 'list',\n",
       "      'description': 'List of previous messages',\n",
       "      'required': False}}},\n",
       "   'llm_text': {'type': 'llm_text',\n",
       "    'element_id': 'llm_text',\n",
       "    'name': 'AI Text Generator',\n",
       "    'description': 'Generates a response using the LLM',\n",
       "    'input_schema': {'prompt': {'type': 'string',\n",
       "      'description': 'The prompt for the LLM',\n",
       "      'required': True},\n",
       "     'context': {'type': 'list',\n",
       "      'description': 'Context for the LLM',\n",
       "      'required': False},\n",
       "     'additional_data': {'type': 'json',\n",
       "      'description': 'Additional data for the LLM',\n",
       "      'required': False}},\n",
       "    'output_schema': {'llm_output': {'type': 'string',\n",
       "      'description': 'Generated text from the LLM',\n",
       "      'required': True}},\n",
       "    'temperature': 0.7,\n",
       "    'max_tokens': 1000,\n",
       "    'wrapper_prompt': 'You are a helpful AI assistant. Please respond to the following: {prompt}\\n\\nContext: {context}, enclose the chain of thought inside <think> Thought </think> tags. and answer within <answer> </answer> tags.'},\n",
       "   'end_node': {'type': 'end',\n",
       "    'element_id': 'end_node',\n",
       "    'name': 'End Block',\n",
       "    'description': 'Exit point of the flow',\n",
       "    'input_schema': {'text_input': {'type': 'string',\n",
       "      'description': 'Text output to return to the user',\n",
       "      'required': True},\n",
       "     'proposed_transaction': {'type': 'json',\n",
       "      'description': 'Transaction to be sent to the blockchain',\n",
       "      'required': False}},\n",
       "    'output_schema': {'text_output': {'type': 'string',\n",
       "      'description': 'Final text output',\n",
       "      'required': True},\n",
       "     'proposed_transaction': {'type': 'json',\n",
       "      'description': 'Final transaction payload',\n",
       "      'required': False}}}},\n",
       "  'connections': [{'from_id': 'start_node', 'to_id': 'llm_text'},\n",
       "   {'from_id': 'chat_input',\n",
       "    'to_id': 'llm_text',\n",
       "    'from_output': 'chat_input',\n",
       "    'to_input': 'prompt'},\n",
       "   {'from_id': 'context_history',\n",
       "    'to_id': 'llm_text',\n",
       "    'from_output': 'context_history',\n",
       "    'to_input': 'context'},\n",
       "   {'from_id': 'llm_text',\n",
       "    'to_id': 'end_node',\n",
       "    'from_output': 'llm_output',\n",
       "    'to_input': 'text_input'}],\n",
       "  'start_element_id': 'start_node',\n",
       "  'metadata': {'name': 'Simple AI Flow',\n",
       "   'description': 'A simple flow that takes user input and generates a response using an LLM'}},\n",
       " 'initial_inputs': {'chat_input': {'chat_input': 'How to write a python script?'},\n",
       "  'context_history': {'context_history': [\"Hello, I'm a harsh\"]}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlowStreamer:\n",
    "    def __init__(self, flow_data, websocket_url=None):\n",
    "        \"\"\"\n",
    "        Initialize the Flow Streamer.\n",
    "        \n",
    "        Args:\n",
    "            flow_data: Dictionary containing flow definition and initial inputs\n",
    "            websocket_url: WebSocket URL to connect to (if None, will be derived from flow_id)\n",
    "        \"\"\"\n",
    "        self.flow_definition    = flow_data.get(\"flow_definition\", {})\n",
    "        self.initial_inputs     = flow_data.get(\"initial_inputs\", {})\n",
    "        self.flow_id            = flow_data.get(\"flow_id\", \"unknown-flow\")\n",
    "        \n",
    "        # Use provided URL or build from flow_id\n",
    "        self.websocket_url      = websocket_url or f\"ws://localhost:8000/ws/execute/{self.flow_id}\"\n",
    "        \n",
    "        # Store of all element data\n",
    "        self.elements = []\n",
    "        self.element_dict = {}  # For quicker lookup by ID\n",
    "        self.execution_order = []\n",
    "        self.llm_chunks = {}  # element_id -> accumulated chunks\n",
    "        \n",
    "    def build_element_dictionary(self):\n",
    "        \"\"\"Build a dictionary of elements from the flow definition\"\"\"\n",
    "        elements = self.flow_definition.get('elements', {})\n",
    "        for element_id, element in elements.items():\n",
    "            element_data = {\n",
    "                'element_id': element_id,\n",
    "                'name': element.get('name', element_id),\n",
    "                'type': element.get('type', 'unknown'),\n",
    "                'description': element.get('description', ''),\n",
    "                'input_schema': element.get('input_schema', {}),\n",
    "                'output_schema': element.get('output_schema', {}),\n",
    "                'inputs': {},\n",
    "                'outputs': {},\n",
    "                'status': 'waiting',\n",
    "                'streamed_data': '',\n",
    "                'start_time': None,\n",
    "                'end_time': None,\n",
    "                'execution_time': None,\n",
    "                'error': None\n",
    "            }\n",
    "            self.elements.append(element_data)\n",
    "            self.element_dict[element_id] = element_data\n",
    "    \n",
    "    def create_payload(self, element_id, name, load, is_end_element=False):\n",
    "        \"\"\"\n",
    "        Create a structured payload for streaming.\n",
    "        \n",
    "        Args:\n",
    "            element_id: ID of the element\n",
    "            name: Name of the element\n",
    "            load: Content to stream\n",
    "            is_end_element: Whether this is an end element payload\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary payload\n",
    "        \"\"\"\n",
    "        if is_end_element:\n",
    "            return {\n",
    "                \"element_id\": element_id,\n",
    "                \"is_end_element\": True,\n",
    "                \"load\": load\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"element_id\": element_id,\n",
    "                \"name\": name,\n",
    "                \"load\": load\n",
    "            }\n",
    "    \n",
    "    def stream_payload(self, payload):\n",
    "        \"\"\"\n",
    "        Stream a payload to the frontend (just prints for now).\n",
    "        \n",
    "        Args:\n",
    "            payload: The payload to stream\n",
    "        \"\"\"\n",
    "        # For now, just print the payload in a structured format\n",
    "        element_id = payload.get(\"element_id\", \"unknown\")\n",
    "        load = payload.get(\"load\", \"\")\n",
    "        \n",
    "        if payload.get(\"is_end_element\"):\n",
    "            print(f\"\\n{element_id} [END]\\n<{load}>\\n{'_' * 40}\")\n",
    "        else:\n",
    "            name = payload.get(\"name\", \"\")\n",
    "            print(f\"\\n{element_id} - {name}\\n<{load}>\\n{'_' * 40}\")\n",
    "        \n",
    "        # In a real implementation, this would send to a frontend:\n",
    "        # await websocket.send(json.dumps(payload))\n",
    "        \n",
    "    def handle_llm_chunk(self, data):\n",
    "        \"\"\"\n",
    "        Handle LLM chunk data.\n",
    "        \n",
    "        Args:\n",
    "            data: Event data containing element_id and content\n",
    "        \"\"\"\n",
    "        element_id = data.get('element_id')\n",
    "        content = data.get('content', '')\n",
    "        \n",
    "        # Print original content to console (as is)\n",
    "        print(content, end='', flush=True)\n",
    "        \n",
    "        # Accumulate chunks for the element\n",
    "        if element_id not in self.llm_chunks:\n",
    "            self.llm_chunks[element_id] = ''\n",
    "            \n",
    "            # Get element name\n",
    "            element_name = \"LLM\"\n",
    "            if element_id in self.element_dict:\n",
    "                element_name = self.element_dict[element_id].get('name', 'LLM')\n",
    "            \n",
    "            # Create and stream initial payload for this LLM element\n",
    "            initial_payload = self.create_payload(\n",
    "                element_id=element_id,\n",
    "                name=element_name,\n",
    "                load=\"[LLM output starting...]\"\n",
    "            )\n",
    "            self.stream_payload(initial_payload)\n",
    "        \n",
    "        self.llm_chunks[element_id] += content\n",
    "        \n",
    "        # Update the element's streamed data\n",
    "        if element_id in self.element_dict:\n",
    "            self.element_dict[element_id]['streamed_data'] = self.llm_chunks[element_id]\n",
    "    \n",
    "    def handle_element_event(self, event_type, data):\n",
    "        \"\"\"\n",
    "        Handle element-related events.\n",
    "        \n",
    "        Args:\n",
    "            event_type: Type of event (element_started, element_completed, etc.)\n",
    "            data: Event data\n",
    "        \"\"\"\n",
    "        element_id = data.get('element_id')\n",
    "        \n",
    "        if event_type == 'element_started':\n",
    "            if element_id in self.element_dict:\n",
    "                element = self.element_dict[element_id]\n",
    "                element['status'] = 'running'\n",
    "                element['start_time'] = datetime.now()\n",
    "                self.execution_order.append(element_id)\n",
    "                \n",
    "                # Create payload with element start info\n",
    "                description = element.get('description', '')\n",
    "                inputs = element.get('inputs', {})\n",
    "                \n",
    "                load = f\"Started: {element.get('name')}\\n\"\n",
    "                if description:\n",
    "                    load += f\"Description: {description}\\n\"\n",
    "                if inputs:\n",
    "                    load += f\"Inputs: {json.dumps(inputs, indent=2)}\"\n",
    "                \n",
    "                payload = self.create_payload(\n",
    "                    element_id=element_id,\n",
    "                    name=element.get('name', ''),\n",
    "                    load=load\n",
    "                )\n",
    "                self.stream_payload(payload)\n",
    "                \n",
    "        elif event_type == 'element_completed':\n",
    "            if element_id in self.element_dict:\n",
    "                element = self.element_dict[element_id]\n",
    "                element['status'] = 'completed'\n",
    "                element['end_time'] = datetime.now()\n",
    "                element['outputs'] = data.get('outputs', {})\n",
    "                \n",
    "                # Calculate execution time\n",
    "                if element['start_time']:\n",
    "                    start = element['start_time']\n",
    "                    end = element['end_time']\n",
    "                    element['execution_time'] = (end - start).total_seconds()\n",
    "                \n",
    "                # Create payload with completion info\n",
    "                outputs = element['outputs']\n",
    "                execution_time = element.get('execution_time', 0)\n",
    "                \n",
    "                load = f\"Completed in {execution_time:.2f} seconds\\n\"\n",
    "                load += f\"Outputs: {json.dumps(outputs, indent=2)}\"\n",
    "                \n",
    "                # If it's an LLM element, include complete streamed data\n",
    "                if element_id in self.llm_chunks:\n",
    "                    payload = self.create_payload(\n",
    "                        element_id=element_id,\n",
    "                        is_end_element=True,\n",
    "                        load=self.llm_chunks[element_id]\n",
    "                    )\n",
    "                else:\n",
    "                    payload = self.create_payload(\n",
    "                        element_id=element_id,\n",
    "                        is_end_element=True,\n",
    "                        load=load\n",
    "                    )\n",
    "                    \n",
    "                self.stream_payload(payload)\n",
    "                \n",
    "        elif event_type == 'element_error':\n",
    "            if element_id in self.element_dict:\n",
    "                element = self.element_dict[element_id]\n",
    "                element['status'] = 'error'\n",
    "                element['end_time'] = datetime.now()\n",
    "                element['error'] = data.get('error', 'Unknown error')\n",
    "                \n",
    "                # Calculate execution time\n",
    "                if element['start_time']:\n",
    "                    start = element['start_time']\n",
    "                    end = element['end_time']\n",
    "                    element['execution_time'] = (end - start).total_seconds()\n",
    "                \n",
    "                # Create payload with error info\n",
    "                error = element['error']\n",
    "                execution_time = element.get('execution_time', 0)\n",
    "                \n",
    "                load = f\"ERROR after {execution_time:.2f} seconds\\n\"\n",
    "                load += f\"Error: {error}\"\n",
    "                \n",
    "                payload = self.create_payload(\n",
    "                    element_id=element_id,\n",
    "                    is_end_element=True,\n",
    "                    load=load\n",
    "                )\n",
    "                self.stream_payload(payload)\n",
    "                \n",
    "    def process_event(self, event):\n",
    "        \"\"\"\n",
    "        Process an event from the websocket.\n",
    "        \n",
    "        Args:\n",
    "            event: Event object from WebSocket\n",
    "        \"\"\"\n",
    "        event_type = event.get('type', '')\n",
    "        data = event.get('data', {})\n",
    "        \n",
    "        # Handle different event types\n",
    "        if event_type == 'llm_chunk':\n",
    "            self.handle_llm_chunk(data)\n",
    "                \n",
    "        elif event_type in ['element_started', 'element_completed', 'element_error']:\n",
    "            self.handle_element_event(event_type, data)\n",
    "                \n",
    "        elif event_type == 'flow_started':\n",
    "            print(f\"\\nFlow {self.flow_id} started at {datetime.now()}\")\n",
    "                \n",
    "        elif event_type == 'flow_completed':\n",
    "            flow_id = data.get('flow_id', self.flow_id)\n",
    "            print(f\"\\nFlow {flow_id} completed at {datetime.now()}\")\n",
    "            \n",
    "        elif event_type == 'flow_error':\n",
    "            error = data.get('error', 'Unknown error')\n",
    "            print(f\"\\nFlow error: {error}\")\n",
    "    \n",
    "    async def stream_flow(self):\n",
    "        \"\"\"\n",
    "        Connect to WebSocket and stream flow execution.\n",
    "        \n",
    "        Returns:\n",
    "            List of element data objects\n",
    "        \"\"\"\n",
    "        # Build element dictionary\n",
    "        self.build_element_dictionary()\n",
    "        \n",
    "        print(f\"Connecting to WebSocket at {self.websocket_url}\")\n",
    "        \n",
    "        try:\n",
    "            async with websockets.connect(self.websocket_url) as websocket:\n",
    "                # Receive ready message\n",
    "                ready_msg = await websocket.recv()\n",
    "                print(f\"Server: {ready_msg}\")\n",
    "                \n",
    "                # Send flow definition\n",
    "                flow_definition_str = json.dumps(self.flow_definition)\n",
    "                await websocket.send(flow_definition_str)\n",
    "                print(\"Sent flow definition\")\n",
    "                \n",
    "                # Receive acknowledgment\n",
    "                ack1 = await websocket.recv()\n",
    "                print(f\"Server: {ack1}\")\n",
    "                \n",
    "                # Send initial inputs\n",
    "                initial_inputs_str = json.dumps(self.initial_inputs)\n",
    "                await websocket.send(initial_inputs_str)\n",
    "                print(\"Sent initial inputs\")\n",
    "                \n",
    "                # Receive acknowledgment\n",
    "                ack2 = await websocket.recv()\n",
    "                print(f\"Server: {ack2}\")\n",
    "                \n",
    "                # Send config (null in this case)\n",
    "                await websocket.send(\"null\")\n",
    "                print(\"Sent null config\")\n",
    "                \n",
    "                # Receive final acknowledgment\n",
    "                ack3 = await websocket.recv()\n",
    "                print(f\"Server: {ack3}\")\n",
    "                \n",
    "                print(\"Flow execution starting. Streaming events...\")\n",
    "                \n",
    "                # Now receive streaming events\n",
    "                try:\n",
    "                    while True:\n",
    "                        message = await websocket.recv()\n",
    "                        event = json.loads(message)\n",
    "                        \n",
    "                        # print (event)\n",
    "                        # print(\"_________________________\")\n",
    "                        \n",
    "                        # Process the event\n",
    "                        self.process_event(event)\n",
    "                        \n",
    "                        # Set inputs for elements (simplified approach)\n",
    "                        if event['type'] == 'element_started':\n",
    "                            element_id = event['data'].get('element_id')\n",
    "                            # Try to find inputs from dependencies that just completed\n",
    "                            for exec_element_id in reversed(self.execution_order):\n",
    "                                if exec_element_id != element_id and self.element_dict[exec_element_id]['status'] == 'completed':\n",
    "                                    # This is a simplification - in a real system, you'd track \n",
    "                                    # the specific input mappings between elements\n",
    "                                    if element_id in self.element_dict:\n",
    "                                        self.element_dict[element_id]['inputs'] = self.element_dict[exec_element_id]['outputs']\n",
    "                                        break\n",
    "                                    \n",
    "                except websockets.exceptions.ConnectionClosed:\n",
    "                    print(\"\\nWebSocket connection closed\")\n",
    "                \n",
    "                # Return the complete elements list\n",
    "                return self.elements\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return self.elements\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def stream_from_dict(flow_data, websocket_url=None):\n",
    "    \"\"\"\n",
    "    Helper function to stream flow execution from a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        flow_data: Dictionary with flow_definition and initial_inputs\n",
    "        websocket_url: Optional WebSocket URL\n",
    "        \n",
    "    Returns:\n",
    "        List of element data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        streamer = FlowStreamer(flow_data, websocket_url)\n",
    "        return await streamer.stream_flow()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error streaming flow: {e}\")\n",
    "        return []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to WebSocket at ws://localhost:8000/ws/execute/simple_ai_flow\n",
      "Server: {\"status\": \"ready\", \"message\": \"Send flow_definition as JSON\"}\n",
      "Sent flow definition\n",
      "Server: {\"status\": \"received_flow\", \"message\": \"Send initial_inputs as JSON\"}\n",
      "Sent initial inputs\n",
      "Server: {\"status\": \"received_inputs\", \"message\": \"Send config as JSON or 'null'\"}\n",
      "Sent null config\n",
      "Server: {\"status\": \"starting\", \"message\": \"Starting flow execution\"}\n",
      "Flow execution starting. Streaming events...\n",
      "\n",
      "Flow simple_ai_flow started at 2025-05-22 04:01:19.927484\n",
      "\n",
      "start_node - Start Block\n",
      "<Started: Start Block\n",
      "Description: Entry point of the flow\n",
      ">\n",
      "________________________________________\n",
      "Error: FlowStreamer.create_payload() missing 1 required positional argument: 'name'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "elements = asyncio.run(stream_from_dict(yaml_data))\n",
    "\n",
    "# # Print number of elements processed\n",
    "# print(f\"\\nProcessed {len(elements)} elements\")\n",
    "\n",
    "# # Print first element as example of the data structure\n",
    "# if elements:\n",
    "#     print(\"\\nExample of element data structure:\")\n",
    "#     example = elements[0]\n",
    "#     print(json.dumps(example, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"\"\"\n",
    "\n",
    "\\n\\nAlright, let\\'s break this down step by step. Here\\'s how to create a Python script for file organization with error handling:\\n\\n1. **Import Required Libraries**:\\n   ```python\\n   import os\\n   import shutil\\n   ```\\n\\n2. **Set Up Directories**:\\n   ```python\\n   source_dir = \"path/to/source\"  # Replace with your directory\\n   ```\\n\\n3. **Process Files**:\\n   ```python\\n   for filename in os.listdir(source_dir):\\n       file_path = os.path.join(source_dir, filename)\\n       \\n       if os.path.isfile(file_path):\\n           # Extract category (e.g., split by \\'_\\')\\n           try:\\n               category = filename.split(\\'_\\')[0]  # Modify based on your naming pattern\\n           except IndexError:\\n               category = \"Miscellaneous\"\\n           \\n           # Create target directory\\n           target_dir = os.path.join(source_dir, category)\\n           os.makedirs(target_dir, exist_ok=True)\\n           \\n           # Move file with error handling\\n           try:\\n               shutil.move(file_path, os.path.join(target_dir, filename))\\n               print(f\"Moved {filename} to {category}/\")\\n           except Exception as e:\\n               print(f\"Error moving {filename}: {str(e)}\")\\n   ```\\n\\n4. **Add Logging** (optional):\\n   ```python\\n   import logging\\n   logging.basicConfig(filename=\\'file_organizer.log\\', level=logging.ERROR)\\n   ```\\n\\n**Key Considerations**:\\n- Test with copies of files first\\n- Adjust filename splitting logic to\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Alright, let's break this down step by step. Here's how to create a Python script for file organization with error handling:\n",
      "\n",
      "1. **Import Required Libraries**:\n",
      "   ```python\n",
      "   import os\n",
      "   import shutil\n",
      "   ```\n",
      "\n",
      "2. **Set Up Directories**:\n",
      "   ```python\n",
      "   source_dir = \"path/to/source\"  # Replace with your directory\n",
      "   ```\n",
      "\n",
      "3. **Process Files**:\n",
      "   ```python\n",
      "   for filename in os.listdir(source_dir):\n",
      "       file_path = os.path.join(source_dir, filename)\n",
      "       \n",
      "       if os.path.isfile(file_path):\n",
      "           # Extract category (e.g., split by '_')\n",
      "           try:\n",
      "               category = filename.split('_')[0]  # Modify based on your naming pattern\n",
      "           except IndexError:\n",
      "               category = \"Miscellaneous\"\n",
      "           \n",
      "           # Create target directory\n",
      "           target_dir = os.path.join(source_dir, category)\n",
      "           os.makedirs(target_dir, exist_ok=True)\n",
      "           \n",
      "           # Move file with error handling\n",
      "           try:\n",
      "               shutil.move(file_path, os.path.join(target_dir, filename))\n",
      "               print(f\"Moved {filename} to {category}/\")\n",
      "           except Exception as e:\n",
      "               print(f\"Error moving {filename}: {str(e)}\")\n",
      "   ```\n",
      "\n",
      "4. **Add Logging** (optional):\n",
      "   ```python\n",
      "   import logging\n",
      "   logging.basicConfig(filename='file_organizer.log', level=logging.ERROR)\n",
      "   ```\n",
      "\n",
      "**Key Considerations**:\n",
      "- Test with copies of files first\n",
      "- Adjust filename splitting logic to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralabs-exec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
